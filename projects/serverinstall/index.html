<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<title>home server based on Ubuntu 20.04 with ZFS</title>
	<meta name="Ubuntu home server" content="home server based on Ubuntu 20.04 with ZFS">
</head>

<body>
	<header>
		<h1>home server based on Ubuntu 20.04 with ZFS</h1>
	</header>
	<nav>
		<ul>
			<li><a href="#introduction">Introduction</a></li>
			<ul>
				<li><a href="#preconditions">Preconditions</a></li>
				<li><a href="#requirements">Requirements</a></li>
				<li><a href="#known_issues">Known issues</a></li>
			</ul>
			<li><a href="#boot_medium">Boot medium creation</a></li>
			<ul>
				<li><a href="#download_ventoy">Download ventoy</a></li>
				<li><a href="#stick_creation">Create bootable USB stick</a></li>
				<li><a href="#persistency">Persistent data area creation</a></li>
				<li><a href="#partitioning">Partition script modification</a></li>
				<li><a href="#part_issue">Partitioning issue</a></li>
			</ul>
			<li><a href="#installation">Installation proces</a></li>
			<ul>
				<li><a href="#additional_hdds">Initialisation of the data pool</a></li>
				<li><a href="#moving_userdata">Moving USERDATA</a></li>
				<li><a href="#moving_serverdata">Moving other datasets</a></li>
				<li><a href="#first_boot">First boot of the fresh system</a></li>
			</ul>
			<li><a href="#configuration">System configuration</a></li>
			<li><a href="#shutdown">Automatic shutdown</a></li>
			<ul>
				<li><a href="#nfs_detection">Detection of nfs mounts</a></li>
				<li><a href="#user_detection">Detection of active users</a></li>
				<li><a href="#updates">Checking for updates</a></li>
				<li><a href="#reboot">Reboot if required</a></li>
				<li><a href="#maintenance">Wake up for maintenance</a></li>
				<li><a href="#all_together">Putting it together</a></li>
			</ul>
		</ul>
	</nav>

	<h1 id="introduction">Introduction</h1>
		<p>
			I am running a home server since many years.
			Somewhere around 2018 I deciced to switch to a new server
			based on XUbuntu 18.04 using ZFS as filesystem. At that time
			I started with a combination of several howtos I found in
			the internet. That was a lot of work and I have no chance to
			list all of the sources as each of them only solved one step
			of the whole story.
		</p>
		<p>
			After switching all of my desktops to XUbuntu 20.04 on ZFS I
			decided to do the same with my server. So I first had to
			check if all things I had to find out to get my requirements
			fullfilled still work on XUbuntu 20.04. And they did.
		</p>

		<h2 id="preconditions">Preconditions</h2>
			<p>
				I don't want to use new hardware, but I have available:
			</p>
			<ul>
				<li>Mainboard: ASROCK B85M-ITX</li>
				<li>Pentium G3440T</li>
				<li>128G SSD for booting</li>
				<li>4 harddisks as raid for data</li>
			</ul>
			<p>
				I personally prefer XUbuntu, so I also need a bootable XUbuntu
				20.04 USB stick.
			</p>

		<h2 id="requirements">Requirements</h2>
			<p>
				I have a list of things my server has to fullfill:
			</p>
			<ul>
				<li>
					In normal case the server shall be off. I don't like current
					consumption when it is not in use.
				</li>
				<li>
					On request it shall be available FAST. So I want to use
					suspend to RAM.
				</li>
				<li>
					On power fail everything shall work smooth, so suspend to
					RAM is not enough - the shutdown has to do a disk image,
					too.
				</li>
				<li>
					The system has to provide Wake On Lan - I don't want to
					push the button.
				</li>
				<li>
					The supension shall be done automatically after some
					minutes of idletime
				</li>
			</ul>

		<h2 id="known_issues">Known issues</h2>
			<ul>
				<li>
					during ubuntu 20.04 installation there is no possibility
					to define the partition layout or to create data pools
					with redundancy. Up to now I found no functional
					modification to create the datapool automatically
					during the regular XUbuntu installation process.
				</li>
				<li>
					(solved) The mainboard is equipped with a Qualcomm Atheros
					AR8171 gigabit ethernet chip. This chip supports Wake On
					Lan, but you have to tweak the linux system to enable it
				</li>
				<li>
					(solved) the ubuntu partitioning when using the
					installation on ZFS creates a 2G swap partition. This is
					not large enough for suspend to disk
				</li>
			</ul>

	<h1 id="boot_medium">Boot medium creation</h1>
		<p>
			I propose to use <a href="www.ventoy.net">Ventoy</a>. It's
			available for Linux and Windows.<br/>
			This page explains the usage of ventoy under linux.<br/>
			Usage under Windows is explained on the website.<br/>
			The configuration of the stick content is identical.
		</p>

		<h2 id="download_ventoy">Download ventoy</h2>
			<p>
				Download the tool here:<br/>
				<a href="https://github.com/ventoy/Ventoy/releases">Ventoy download from Github</a><br/>
				You'll get a file "ventoy-x.y.z-linux.tar.gz" with x, y, and z
				digits for the version.
			</p>
			<p>
				Optional: check the SHA256 against the value given on the
				website.<br/>
				<code>sha256sum ventoy-x.y.z-linux.tar.gz</code><br/>
				and compare the value.<br/>
				don't forget to use the <em>real</em> filename
			</p>
			<p>
				Extract the file with the command<br/>
				<code>tar -xaf ventoy-x.y.z-linux.tar.gz</code><br/>
				don't forget to use the <em>real</em> filename
			</p>

		<h2 id="stick_creation">Create bootable USB stick</h2>
			<p>
				cd into the extracted directory<br/>
				<code>cd ventoy-x.y.z</code><br/>
				don't forget to use the <em>real</em> directory name
			</p>
			<p>
				Insert an USB stick with enough space for the ubuntu iso file
				plus a file for persistent data. I recommend to use a stick
				with at least 8GB.<br/>
			</p>
			<p>
				run ventroy with the command<br/>
				<code>sudo sh Ventoy2Disk.sh -I /dev/XXX</code><br/>
				Replace XXX by the device name of your USB stick.
			</p>
			<p>
				Your computer will remount the stick and show one partition
				named<br/>
				<code>ventoy</code>
			</p>
			<p>
				Copy your ubuntu iso file on this USB stick.<br/>
				Now the stick is bootable, but we need the possibility to
				modify files on the stick persistent.
			</p>

		<h2 id="persistency">Persistent data area creation</h2>
			<p>
				ventoy contains a plugin for persistent data creation on
				bootable USB sticks. To prapare this execute the command<br/>
				<code>sudo sh CreatePersistentImg.sh -s 1024 -t ext4 -l casper-rw</code>
			</p>
			<p>
				This creates a file <code>persistence.dat</code> in your actual
				directory.<br/>
				Copy the file on the stick.
			</p>
			<p>
				Now we have to tell ventoy to use this persistence file for
				booting ubuntu.<br/>
				To do so create a directory <code>ventoy</code>
				on the USB stick.
			</p>
			<p>
				Create a file <code>ventoy.json</code> in this directory.<br/>
				Content of this file has to be:
			</p>
<pre>
{
	"persistence": [
		{
			"image": "/xubuntu-20.04.1-desktop-amd64.iso",
			"backend": "/persistence.dat",
			"autosel": 1
		}
	]
}
</pre>
			<p>
				You can adapt the file names to your needs. It's also possible
				to rename <code>persistence.dat</code>, in that case use the
				new filename in <code>ventoy.json</code>, too.
			</p>

		<h2 id="partitioning">Partition script modification</h2>
			<p>
				As mentioned above the default partitioning script of the
				ubuntu installation medium creates a swap partition of 2G.
				If the system shall do suspend to disk the swap partition is
				used to store the complete RAM content. Knowing this leads to
				the conclusion that 2G is - in most case - much to small.
			</p>
			<p>
				As solution I first do a modification on the USB stick. The
				stick creation process explained above leads to a persistent
				storage in that boot system. So I boot the live stick and
				change the required file. Since then this modified file is used
				for partitioning.
			</p>
			<p>
				After booting your future server system from the USB stick
				with "Try without installation" patch the file
				<code>/usr/share/ubiquity/zsys-setup</code>
				with root rights using this patch file:
				<a href="zsys-setup.patch">zsys-setup.patch</a>
			</p>
			<p>
				to apply the patch open a terminal and type
<pre>
wget https://andiweiss.github.io/projects/serverinstall/zsys-setup.patch
cd /usr/share/ubiquity
sudo patch -p1 &lt; /home/xubuntu/zsys-setup.patch
</pre>
			</p>
			<p>
				This patch modifies the zfs initialization script in a way that
				the swap partition is as large as the physical ram of the machine.
			</p>
			<p>
				After applying the patch the stick is prepared for installation.
				You can start the installation either with a new boot or without
				rebooting by using the install ubuntu icon.
			</p>
			<p>
				If you regulary work in Linux it's easy to have a look on that
				file on another linux machine. After saving the file in the
				life system shut down the life system and connect the USB stick
				to your working linux system. In a default environment your
				system will mount the stick under <code>/media/&lt;user&gt;/ventoy</code>
			</p>
			<p>
				create a directory for temporary mounting of the persitence file
<pre>
mkdir pers
</pre>
			</p>
			<p>
				Now mount the persistence file
<pre>
sudo mount /dev/media/&lt;user&gt;/ventoy/persistence.dat pers
</pre>
				In the case that you have chosen a different filename use that one.
			</p>
			<p>
				Now you have access to your modified zsys-setup. You find it in
				<code>pers/upper/usr/share/ubiquity/zsys-setup</code>
			</p>
		<h2 id="part_issue">Partitioning issue</h2>
			<p>
				I tried hard to get the installation doing all the things for having
				a separate data tank. But it always failed on grub installation.
				That's the reason why this howto just changes the swap partition size
				and does the additional pool handling after base installation but before
				starting the installed system the first time.
			</p>
						
	<h1 id="installation">Installation process</h1>
		<p>
			I recommend doing the installation out of the life system, not the
			<code>install xubuntu</code> possibility.
		</p>
		<p>
			Reason for that is to be able to add the data device before booting
			into the new installed system. The actual installer does not give the
			possibility to add an additional zfs pool with different features
			like mirroring or raid. The actual installer just puts everything on
			one hard disk / ssd.
		</p>
		<p>
			<em>Do the installation with only the future boot disk connected!</em>
		</p>
		<p>
			Do the installation in the regular way.
			We take care for moving the user data later.
		</p>
		<p>
			Just keep care for one Window:
		</p>
		<p>
			In the Window "Installation type" you have to chose "Advanced
			Features ...". In the now opened window select "EXPERIMENTAL:
			Erase disk and use ZFS". Then select the device you want to boot
			from.
		</p>
		<p>
			Now let the installer do its work. After that you'll have an
			ubuntu system on one hard disk using ZFS wherever it is
			possible. There's only one partition with another file system.
			The EFI partition has to be formatted as VFAT.
		</p>
		<p>
			After the installation process has been finished the system asks
			for a reboot. I recommend not to do that directly but instead
			prepare the HDDs you want to use for the data pool.
		</p>
		<h2 id="additional_hdds">Initialisation of the data pool</h2>
			<p>
				After the completion of the basic installation process
				I hotplug the harddisk or harddisks for the server data pool
				while still running the life system used for installation.
				After the disks have been detected I use a commandline shell
				to create the pool and the required data sets.
			</p>
			<p>
				First step: import the already existing pools:
			</p>
<pre>
sudo zpool import -R /target rpool
sudo zpool import -R /target bpool
</pre>
			<p>
				Second step: create the data tank. For pool creation I use nearly
				the same parameters as used for rpool creation:
<pre>
sudo zpool create -f \
	-o ashift=12 \
	-o autotrim=on \
	-O compression=lz4 \
	-O acltype=posixacl \
	-O xattr=sa \
	-O relatime=on \
	-O normalization=formD \
	-O mountpoint=none \
	-O canmount=off \
	-O dnodesize=auto \
	-O sync=disabled \
	tank0 /dev/disk/by-id/&lt;your_id_here&gt;
</pre>
				Checking this against the creation of <code>rpool</code> you'll face only
				two differences:
				<ul>
					<li>the mount point is set to <code>none</code></li>
					<li>no alternative mount point is defined (no <code>-R</code> is given)</li>
				</ul>
				If the data pool shall have mirroring, raid or other fancy
				modes just hotplug the required disks andadapt the parameters to your needs.
				Please keep in mind:
			</p>
			<p>
				<em>DO NOT</em> use <code>/dev/sd*</code> for the pool creation!
			</p>
			<p>
				Check the entries in <code>/dev/disk/by-id/</code> for the drives you
				want to use and use these links instead.
			</p>
			<p>
				By handing over the whole device zpool will first create a gpt partition
				table, add two partitions - one with nearly the whole content plus
				a small one containing some MB additional space. This is done to be able to
				combine drives with small differences in the amount of sectors to one pool.
				The large partition is used for zfs afterwards.
			</p>
		<h2 id="moving_userdata">Moving USERDATA</h2>
			<p>
				Third step is moving the initial user dataset. To do so we create a
				snapshot of the base installed user data set and move this to
				the new data pool.
<pre>
sudo zfs snapshot -r rpool/USERDATA@move
sudo zfs send -R rpool/USERDATA@move | sudo zfs receive tank0/USERDATA
</pre>
				Now the snapshot is available on the additional pool. This snapshot contains
				all properties of the original dataset. Parts of the properties are required
				to have the regular ubuntu zfs mechanisms working.
			</p>
			<p>
				Now the original data set can be destroyed:
<pre>
sudo zfs destroy -r rpool/USERDATA
</pre>
				If you only want to use the user data on the data tank you're done with the
				data movement. If you want to have server data follow the next steps and add more
				dependent on your requirements.
			</p>
		<h2 id="moving_serverdata">Moving other datasets</h2>
			<p>
				I want to have <code>/srv</code> and <code>/var/www</code>
				on my data tank.
			</p>
			<p>
				Fourth step: moving /srv on the data tank. Comparable procedure as with the
				user data, but here the installation process uses an id in the created data sets.
				So take care to replace &lt;ID&gt; by the id used in your instalation process.
				You can read it with <code>zfs list</code>
<pre>
sudo zfs snapshot -r rpool/ROOT/ubuntu_&lt;ID&gt;/srv@move
sudo zfs send -R rpool/ROOT/ubuntu_&lt;ID&gt;/srv@move | sudo zfs receive tank0/srv
sudo zfs destroy -r rpool/ROOT/ubuntu_&lt;ID&gt;/srv
</pre>
				As the mountpoint in the original dataset is inherited from the upper dataset
				we have to set the correct mountpoint here:
<pre>
sudo zfs set mountpoint=/srv tank0/srv
</pre>
			</p>
			<p>
				Fifth step: As mentioned I also want to get <code>/var/www</code> on the tank.
				There is only one difference to moving the <code>/srv</code>: I want to create
				<code>/var</code> first and then put <code>/var/www</code> on top. Just for having
				a better overview.
			</p>
			<p>
				So this is the command list for doing so:
<pre>
sudo zfs snapshot -r rpool/ROOT/ubuntu_&lt;ID&gt;/var/www@move
sudo zfs create tank0/var -o canmount=off -o mountpoint=none
sudo zfs send -R rpool/ROOT/ubuntu_&lt;ID&gt;/var/www@move | sudo zfs receive tank0/var/www
sudo zfs destroy -r rpool/ROOT/ubuntu_&lt;ID&gt;/var/www
sudo zfs set mountpoint=/var/www tank0/var/www
</pre>
			</p>
			<p>
				Now we have to introduce the new pool to the system as bpool and rpool are.<br/>
<pre>
sudo zpool set cachefile= tank0
sudo touch /target/etc/zfs/zfs-list.cache/tank0
sudo zfs set sync=standard tank0
</pre>
			</p>
			<p>
				Having this done tank0 is ready for usage, so now we export all pools
				in the opposite order as we imported / created them:
<pre>
sudo zpool export tank0
sudo zpool export bpool
sudo zpool export rpool
</pre>
			</p>

		<h2 id="first_boot">First boot of the fresh system</h2>
			<p>
				The system has to be rebooted. You can do that fastest with
				<code>sudo reboot</code><br/>
				You will be requested to remove the installation medium and hit enter.
			</p>
			<p>
				After doing so your system will <em>not</em> boot completly.<br/>
				Up to now it doesn't know that it has to import the new created data sets.
				But it requires these sets. So you will reach the emergency state.
			</p>
			<p>
				Hit enter again to reach a command line.<br/>
				Then type
<pre>
zpool import tank0
exit
</pre>
				Now your system boots first time into the fresh installation.
			</p>

	<h1 id="configuration">System configuration</h1>
		<p>
			The initial system installation is deon now. There will be updates
			available - these can either be installed first or last. The result
			will be the same.
		</p>

		<h2>ssh server</h2>
			<p>
				Now the system has a state where I want to get remote access.
				Therefore my next step is the installation of the ssh server.
<pre>
sudo apt install openssh-server
</pre>
			</p>
			<p>
				In the case you want to replace an old server it may make sense
				to overtake the ssh keys from the old machine. To do so copy
				the files <code>/etc/ssh/*key*</code> from the old machine to
				the new one.
			</p>

		<h2>Small configuration issues</h2>
			<p>
				After the ubuntu installation process is finished we have a
				clean installation waiting for further configuration. There is
				one thing I do not like at all:<br/>
				There are icons for rpool and bpool on the desktop I want to
				get rid of.
			</p>
			<p>
				To get rid of these icons we have to enter two lines in
				<code>/etc/fstab</code><br/>
				This file has to be editied with root rights. Just add the two
				lines
<pre>
LABEL=rpool	none	auto	noauto	0	0
LABEL=bpool	none	auto	noauto	0	0
</pre>
			</p>
			<p>
				One thing I really like is the possibility to use pageup /
				pagedown for searching the history.<br/>
				This can be enabled in <code>/etc/inputrc</code>
			</p>
			<p>
				As last little config issue I disable the bluetooth service. It
				usually crashes during boot and I don't have a bluetooth device
				connected to the server.
<pre>
sudo systemctl disable bluetooth
</pre>
			</p>

		<h2>Configuration of suspend and hibernate</h2>
			<p>
				Now the basics are done. We have a system booting from ZFS and
				the desktop is clean.
			</p>
			<p>
				Next step to do is the configuration of suspend to RAM and
				suspend to disk.
			</p>
			<p>
				To get supend, hibernate and a combination of both full
				functional we just have to install one official ubuntu package:
<pre>
sudo apt install uswsusp
</pre>
			</p>
			<p>
				Having this package installed there is nothing to say against a
				test of suspend to ram, disk or both.
			</p>
			<p>
				To test suspend to RAM execute the command
<pre>
sudo s2ram
</pre>
			</p>
			<p>
				If everything is fine the system shuts down very fast.
				At least my system wakes up with hitting any key on the
				keyboard. Everything is fine if you don't get the regular login
				but the screen saver password request.
			</p>
			<p>
				Next try is suspend to disk. The command for doing that is
<pre>
sudo s2disk
</pre>
			</p>
			<p>
				In this case shuting down needs a bit more time and the system
				prints additional information on the screen where the image is
				stored and how large it is. On boot it takes the bios time,
				then loads the image and again there should be the screensaver
				password request. If you get the regular login screen something
				went wrong.
			</p>
			<p>
				Last test for me is testing if the combination suspend to RAM
				and to disk is also working fine. The related command is
<pre>
sudo s2both
</pre>
			</p>
			<p>
				After this the shut down looks simular as in <code>s2disk</code>.
				Shutting down needs the same time as in <code>s2disk</code>.
				But as long as you don't pull the power cord the system will
				start up in the same time as with <code>s2disk</code>.<br/>
				<em>If</em> you pull the power cord and plug it again the system
				will again recover in the same state where you called
				<code>s2both</code>.
			</p>
			<p>
				To have this as a sensefull feature you have to setup your BIOS
				in the way that after power loss the computer starts up. In most
				cases the default for this setting is "keep it off".
			</p>

		<h2>Configuration of Wake On Lan</h2>
			<p>
				As already explained in the prerequesites the Atheros AR8171 in
				principle supports Wake On Lan, but it is completely disabled
				in the ubuntu standard installation. So we're going to activate
				it now.
			</p>
			<p>
				First thing to do is to install ethtool:
<pre>
sudo apt install ethtool
</pre>
			</p>
			<p>
				Now we need to know which interface we want to configure:
<pre>
ip link
</pre>
				We identify the interface, in my case this is <code>enp2s0</code>
			</p>
			<p>
				Next step is to check the supported wake on lan modes and the
				actually configured ones:
<pre>
sudo ethtool enp2s0 | grep Wake-on
</pre>
				On my server hardware this reports:
<pre>
Supports Wake-on: d
Wake-on: d
</pre>
			</p>
			<p>
				This is sad, because this means: No Wake On Lan supported ...
			</p>
			<p>
				<em>But:</em> the driver supports WOL, it just has to be
				enabled. This is done by an additional kernel command line
				parameter.
			</p>
			<p>
				To get WOL working on the Atheros AR8171 the kernel requires
				the additional commandline paramter <code>alx.enable_wol=1</code>.
				To add this parameter edit the file <code>/etc/default/grub</code>
				with root rights. Here you find a line
<pre>
GRUB_CMDLINE_LINUX_DEFAULT="&lt;some_parameters&gt;"
</pre>
				in this line add <code>alx.enable_wol=1</code> with  leading
				blank in front of the second quotation mark. After doing that
				execute the command
<pre>
sudo update-grub
</pre>
				to overtake the change.
			</p>
			<p>
				Now execute a reboot, open a terminal and use the command
<pre>
sudo ethtool enp2s0 | grep Wake-on
</pre>
				again.
			</p>
			<p>
				The new result is:
<pre>
Supports Wake-on: pg
Wake-on: pg
</pre>
			</p>
			<p>
				So now Wake On Lan is activated on
				<ul>
					<li>phy activity</li>
					<li>MagicPacket</li>
				</ul>
				I don't like phy activity waking up my system, so I have to
				use ethtool to change that behavior. Changing it in the
				commandline will be lost after reboot, therefore I add a task
				in systemd.
			</p>
			<p>
				To create that service create the file
				<code>/etc/systemd/system/wol.service</code>
				with root rights. This file has to contain:
<pre>
[Unit]
Description=Configure Wake-up on LAN

[Service]
Type=oneshot
ExecStart=/sbin/ethtool -s enp2s0 wol g

[Install]
WantedBy=basic.target
</pre>
				Then enable the service with
<pre>
sudo systemctl enable wol.service
</pre>
			</p>
			<p>
				After a reboot ethtool reports
<pre>
Supports Wake-on: pg
Wake-on: g
</pre>
				<em>This</em> is what we want to get.
			</p>
			<p>
				Testing the feature can be done with another linux system.
				I recommend to use wakeonlan which has to be installed with
<pre>
sudo apt install wakeonlan
</pre>
				on another linux machine in the same network.
			</p>
			<p>
				The previous used command <code>ip link</code> does not only
				print the interface name. It also prints the mac address. The
				mac address is the part behind <code>link/ether</code>,
				containing six hexadecimal values with 2 digits each, the values
				separated by colons.
			</p>
			<p>
				To try if wake on lan is functional put your server to sleep
				e.g. with <code>sudo s2ram</code>. Then, on the other machine,
				use
<pre>
wakeonlan 11:22:33:44:55:66
</pre>
				to wake it up again.
				Take care to use the mac address of your server. Another thing
				you need to know is that the magic packet used for waking up a
				system is not routed over wifi, so you have to use a computer
				connected to the lan.
			</p>

		<h2>exporting nfs shares</h2>
			<p>
				Now we have a system providing the basic system features.
				Now the server featurs have to be activated.
				I prefer haveing nfs shares, so I don't take cifs shared into
				account here.
			</p>
			<p>
				First thing to prepare nfs shares is the installtion of the
				nfs server:
<pre>
sudo apt install nfs-kernel-server
</pre>
			</p>
			<p>
				I decided to use the zfs mechanisms for exporting nfs shares.
				Therefore exporting the filesystems is done with zfs properties,
				not in <code>/etc/exports</code>.
			</p>
			<p>
				I'm going to show the export with my folder containing the mp3
				collection. This is located in <code>/srv/music</code>
			</p>
			<p>
				First I have to create that dataset:
<pre>
sudo zfs create tank0/srv/music
</pre>
				By creating this dataset there's automatically a directory
				<code>/srv/music</code>
				available. Take care to set the access (user, group, read,
				write, execute, ...) according your needs.
			</p>
			<p>
				Now we simply tell zfs to export this filesystem:
<pre>
sudo zfs set sharenfs=on tank0/srv/music
</pre>
				After doing so this share can be mounted on another linux
				machine:
<pre>
sudo mount server:/srv/music music
</pre>
			</p>

	<h1 id="shutdown">Automatic shutdown</h1>
		<p>
			Now the basic system is complete.
			User can be added and their home directory will be created
			on the tank device. Suspend to ram and disk is working.
			Wake on LAN is also full functional.
			So the next thing is to shutdown the system automatically.
		</p>
		<p>
			Actually I see two things which have to keep the system alive:
			<ul>
				<li>any nfs share in use</li>
				<li>a user logged in via ssh or in the terminal</li>
			</ul>
		</p>
		<p>
			But before shutting down the system it shall be checked if there are
			updates. If yes these updates shall be installed. And if - after
			this update - the system requires a reboot this shall be done, too.
		</p>

		<h2 id="nfs_detection">Detection of nfs mounts</h2>
			<p>
				To detect how many nfs mounts are actually in use I use the
				program netstat. This is included in the ubuntu package
				net-tools, which is not installed as default. So we have to install it:
<pre>
sudo apt install net-tools
</pre>
			</p>
			<p>
				To get a list of active nfs connections I use
<pre>
netstat -n --inet
</pre>
				I use the <code>-n</code> to get numeric output. Otherwise there has
				to be a functional nameserver in your network. Additionally we need
				the ip address of the client lateron.
			</p>
			<p>
				This command does not only print the nfs connections, it prints all
				active tcp and udp connections. So we have to filer for nfs.
			</p>
			<p>
				Taking a look into <code>/etc/services</code> we find the ports used for
				nfs. This is 2049. With this I use
<pre>
netstat -n --inet | grep ':2049\W'
</pre>
				to get only the lines with active nfs connections.
			</p>
			<p>
				During my tries I found an issue for generic usage:<br/>
				If the system using the share doesn't unmount the share it can happen that
				netstat detects the link active for a long time. So we also have to check if
				the system with the actual link is still alive.
			</p>
			<p>
				For the correct handling of this we need a script.
				Here I don't quote the script, you'll find it where the cron job is explained.
				I'll just explain how that works.
			</p>
			<p>
				As long as the system is alive we expect that it can be ping'ed.
				This depends if there is a firewall installed which blocks the ping,
				on my system this is not the case. So I can use ping.
			</p>
			<p>
				To be able to ping the system we need its ip address.
				To get this we use the output of the found line of netstat and kill everything but the ip address:
<pre>
netstat -n --inet | grep ':2049\W' | sed -n 's|.*:2049\W*\([^:]*\):.*|\1|g;p'
</pre>
			</p>

		<h2 id="user_detection">Detection of active users</h2>
			<p>
				Getting the information if users are logged in is easy:
<pre>
users
</pre>
				prints all users who are logged in.
			</p>

		<h2 id="updates">Checking for updates</h2>
			<p>
				Easiest thing is not checking for updates but just doing the updates.
				To do so we tell the system to
				<ul>
					<li>get the new package list</li>
					<li>install the security updates</li>
					<li>install the updates which don't require interaction</li>
					<li>do an autoclean</li>
				</ul>
			</p>

		<h2 id="reboot">Reboot if required</h2>
			<p>
				The update process creates a file <code>/var/run/reboot-required</code>
				if a reboot is required. If this file is detected the server will - instead of
				go to suspend - do a reboot.
			</p>

		<h2 id="maintenance">Wake up for maintenance</h2>
			<p>
				In my case the server is often not used for a long time.
				To avoid having a too long tim without updates the system
				shall wake up at least once a week and do maintenance.
			</p>
			<p>
				To get that there is a tool called <code>rtcwakeup</code> which
				uses the rtc alarm to start the system at a defined time.
				My decission is to start the system at the same day in one week
				at 4 o'clock in the morning.
			</p>
			<p>
				This is done in the script before shutting down the system
				with the command
<pre>
rtcwake -m no -t `date --date='7 days 04:00' +%s`
</pre>
				This sets the alarm in the rtc which then will start up the system.
			</p>

	<h2 id="all_together">Putting it together</h3>
		<p>
			Now we know how to get the information if we have to shut down.
			These things have to be used for putting it all together.
			We need to know if a user is actually logged in and if a share is
			actually in use.<br/>
			When this is not the case for a certain time the system shall shut down.
		</p>
		<p>
			As this is time based we create a cron job for that.
			In the cron job, once per minute, we collect the information.
			Then we store the information when we first have no keep alive
			status anymore. If this doesn't change for the defined time we
			do a <code>s2both</code> and the system gets suspended.
		</p>
		<p>
			I provide a script for the automatic shutdown:<br/>
			<a href="autosuspend.sh">autosuspend.sh</a><br/>
			Save this script with root rights as <code>/bin/autosuspend.sh</code>
			and don't forget to <code>sudo chmod +x /bin/autosuspend.sh</code>
			to make it executable.
<pre>
wget https://AndiWeiss.github.io/projects/serverinstall/autosuspend.sh
chmod a+x autosuspend.sh
sudo chown root:root autosuspend.sh
sudo mv autosuspend.sh /bin
</pre>
			After that we create a cronjob calling this script every minute.
			Therefore open your <code>/etc/crontab</code> with root rights
			and add the line
<pre>
* * * * * root /bin/autosuspend.sh
</pre>
			With this the script is executed each minute and then checks
			if the system shall stay active or not.
		</p>
</body>
</html>
<!--
sudo apt install uswsusp ethtool nfs-kernel-server net-tools 
-->
