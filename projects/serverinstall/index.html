<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<title>home server based on Ubuntu 22.04 with ZFS</title>
	<meta name="Description" content="linux home server, zfs, wake on lan, Qualcomm Atheros QCA8171, auto suspend"/>
</head>

<body>
	<header>
		<h1>home server based on Ubuntu 22.04 with ZFS</h1>
	</header>
	<nav>
		<ul>
			<li><a href="#introduction">Introduction</a></li>
			<ul>
				<li><a href="#preconditions">Preconditions</a></li>
				<li><a href="#requirements">Requirements</a></li>
				<li><a href="#known_issues">Known issues</a></li>
			</ul>
			<li><a href="#boot_medium">Boot medium creation</a></li>
			<ul>
				<li><a href="#download_ventoy">Download ventoy</a></li>
				<li><a href="#stick_creation">Create bootable USB stick</a></li>
				<li><a href="#persistency">Persistent data area creation</a></li>
				<li><a href="#partitioning">Partition script modification</a></li>
				<li><a href="#part_issue">Partitioning issue</a></li>
			</ul>
			<li><a href="#installation">Installation process</a></li>
			<ul>
				<li><a href="#additional_hdds">Initialisation of the data pool</a></li>
				<li><a href="#moving_userdata">Moving USERDATA</a></li>
				<li><a href="#moving_serverdata">Moving other datasets</a></li>
				<li><a href="#first_boot">First boot of the fresh system</a></li>
			</ul>
			<li><a href="#configuration">System configuration</a></li>
			<ul>
				<li><a href="#small_issues">Small configuration issues</a></li>
				<li><a href="#supend_hibernate_config">Configuration of suspend and hibernate</a></li>
				<li><a href="#wol_config">Configuration of Wake On Lan</a></li>
				<li><a href="#share_export">exporting nfs shares</a></li>
			</ul>
			<li><a href="#shutdown">Automatic shutdown</a></li>
			<ul>
				<li><a href="#nfs_detection">Detection of nfs mounts</a></li>
				<li><a href="#user_detection">Detection of active users</a></li>
				<li><a href="#updates">Checking for updates</a></li>
				<li><a href="#reboot">Reboot if required</a></li>
				<li><a href="#maintenance">Wake up for maintenance</a></li>
				<li><a href="#all_together">Putting it together</a></li>
			</ul>
			<li><a href="#crash_recovery">Crash recovery</a></li>
			<ul>
				<li></li>
			</ul>
		</ul>
	</nav>

	<h1 id="introduction">Introduction</h1>
		<p>
			I am running a home server since many years.
			Somewhere around 2018 I deciced to switch to a new server
			based on XUbuntu 18.04 using ZFS as filesystem. At that time
			I started with a combination of several howtos I found in
			the internet. That was a lot of work and I have no chance to
			list all of the sources as each of them only solved one step
			of the whole story.
		</p>
		<p>
			After switching all of my desktops to XUbuntu 20.04 on ZFS I
			decided to do the same with my server. So I first had to
			check if all things I had to find out to get my requirements
			fullfilled still work on XUbuntu 20.04. And they did.
		</p>
		<p>
			But time went by and meanwhile Ubuntu 22.04 is available.
			So again I wanted to get everything on the same basis and tried
			the installation on the server with this LTS version.
			Sadly I struggled over some things:
			<ul>
				<li>Wake On Lan didn't work anymore</li>
				<li>Ubuntu 22.04 doesn't install zsys anymore</li>
			</ul>
			After reading several problem reports on the zsys daemon I
			decided not to install zsys - so the nice automatic rollback
			feature is gone ...
		</p>
		<p>
			This site <a href="https://github.com/ubuntu/zsys/issues/230">Don't use ZSYS</a>
			explains why I decided not to use zsys anymore.<br>
			I don't like to have known bugs on the server which may lead to a complete
			removal of my user dataset ...
		</p>
		<p>
			Additionally my boot disc changed.<br>
			Instead of using an eSATA configuration I put a NVME drive
			into a USB3 housing which supports the trim feature and
			connect this to one of the USB3 ports.
		</p>

		<h2 id="preconditions">Preconditions</h2>
			<p>
				I don't want to use new hardware, but I have available:
			</p>
			<ul>
				<li>Mainboard: ASROCK B85M-ITX</li>
				<li>Pentium G3440T</li>
				<li>256G NVME for booting</li>
				<li>NVME USB3 housing supporting trim</li>
				<li>4 harddisks as raid for data</li>
			</ul>
			<p>
				I personally prefer XUbuntu, so I also need a bootable XUbuntu
				22.04 USB stick.
			</p>

		<h2 id="requirements">Requirements</h2>
			<p>
				I have a list of things my server has to fullfill:
			</p>
			<ul>
				<li>
					In normal case the server shall be off.<br>
					I don't like current consumption when it is not in use.
				</li>
				<li>
					On request it shall be available FAST. So I want to use
					suspend to RAM.
				</li>
				<li>
					On power fail everything shall work smooth,<br>
					so suspend to RAM is not enough - the shutdown has to do
					a disk image, too.
				</li>
				<li>
					The system has to support Wake On Lan<br>
					I don't want to push the button.
				</li>
				<li>
					The suspension shall be done automatically after some
					minutes of idletime
				</li>
			</ul>

		<h2 id="known_issues">Known issues</h2>
			<ul>
				<li>
					during ubuntu 22.04 installation there is no possibility
					to define the partition layout or to create data pools
					with redundancy. Up to now I found no functional
					modification to create the datapool automatically
					during the regular XUbuntu installation process.
				</li>
				<li>
					(solved) The mainboard is equipped with a Qualcomm Atheros
					AR8171 gigabit ethernet chip. This chip supports Wake On
					Lan, but that feature is not in Ubuntu since kernel 5.15.<br>
					I created a dkms packages to patch the kernel and build
					the module on kernel update.
				</li>
				<li>
					(solved) the ubuntu partitioning when using the
					installation on ZFS creates a 2G swap partition. This is
					not large enough for suspend to disk
				</li>
			</ul>

	<h1 id="boot_medium">Boot medium creation</h1>
		<p>
			I propose to use <a href="https://www.ventoy.net">Ventoy</a>. It's
			available for Linux and Windows.<br/>
			This page explains the usage of ventoy under linux.<br/>
			Usage under Windows is explained on the website.<br/>
			The configuration of the stick content is identical.
		</p>

		<h2 id="download_ventoy">Download ventoy</h2>
			<p>
				Download the tool here:<br/>
				<a href="https://github.com/ventoy/Ventoy/releases">Ventoy download from Github</a><br/>
				You'll get a file "ventoy-x.y.z-linux.tar.gz" with x, y, and z
				digits for the version.
			</p>
			<p>
				<em>Hint:</em><br>
				I faced an issue when trying to boot xubuntu 22.04.1
				with Ventoy version 1.0.81 in UEFI mode!<br>
				Others had that issue with Ventoy version 1.0.78. I tried 1.0.79 and it works.
			</p>
			<p>
				Optional: check the SHA256 against the value given on the
				website.<br/>
				<code>sha256sum ventoy-x.y.z-linux.tar.gz</code><br/>
				and compare the value.<br/>
				don't forget to use the <em>real</em> filename
			</p>
			<p>
				Extract the file with the command<br/>
				<code>tar -xaf ventoy-x.y.z-linux.tar.gz</code><br/>
				don't forget to use the <em>real</em> filename
			</p>

		<h2 id="stick_creation">Create bootable USB stick</h2>
			<p>
				cd into the extracted directory<br/>
				<code>cd ventoy-x.y.z</code><br/>
				don't forget to use the <em>real</em> directory name
			</p>
			<p>
				Insert an USB stick with enough space for the ubuntu iso file
				plus a file for persistent data.
				<br>I recommend to use a stick
				with at least 8GB.
			</p>
			<p>
				run ventroy with the command<br/>
				<code>sudo sh Ventoy2Disk.sh -I /dev/XXX -g</code><br/>
				Replace XXX by the device name of your USB stick.<br>
				The parameter <code>-g</code> creates a gpt partition scheme on the stick.<br>
				This stick can be booted in oldstyle BIOS mode or in UEFI mode.
			</p>
			<p>
				Your computer will remount the stick and show one partition
				named<br/>
				<code>Ventoy</code>
			</p>
			<p>
				Copy your ubuntu iso file on this USB stick.<br/>
				Now the stick is bootable, but we need the possibility to
				modify files on the stick persistent.
			</p>

		<h2 id="persistency">Persistent data area creation</h2>
			<p>
				ventoy contains a plugin for persistent data creation on
				bootable USB sticks.
			</p>
			<p>
				In general that's a good idea, but I faced
				problems with firefox in the lifelinux when activating the
				persistant data storage.<br>
				Nevertheless I explain how this can be activated.
			</p>
			<p>
				To prepare this execute the command<br/>
				<code>sudo sh CreatePersistentImg.sh -s 1024 -t ext4 -l casper-rw</code>
			</p>
			<p>
				This creates a file <code>persistence.dat</code> in your actual
				directory.<br/>
				Copy the file on the stick.
			</p>
			<p>
				Now we have to tell ventoy to use this persistence file for
				booting ubuntu.<br/>
				To do so create a directory <code>ventoy</code>
				on the USB stick.
			</p>
			<p>
				Create a file <code>ventoy.json</code> in this directory.<br/>
				Content of this file has to be:
			</p>
<pre>
{
	"persistence": [
		{
			"image": "/xubuntu-22.04.1-desktop-amd64.iso",
			"backend": "/persistence.dat",
			"autosel": 1
		}
	]
}
</pre>
			<p>
				You can adapt the file names to your needs. It's also possible
				to rename <code>persistence.dat</code>, in that case use the
				new filename in <code>ventoy.json</code>, too.
			</p>

		<h2 id="partitioning">Partition script modification</h2>
			<p>
				As mentioned above the default partitioning script of the
				ubuntu installation medium creates a swap partition of 2G.
				If the system shall do suspend to disk the swap partition is
				used to store the complete RAM content. Knowing this leads to
				the conclusion that 2G is - in most case - much to small.
			</p>
			<p>
				As solution I first do a modification on the USB stick. The
				stick creation process explained above leads to a persistent
				storage in that boot system. So I boot the live stick and
				change the required file. Since then this modified file is used
				for partitioning.
			</p>
			<p>
				After booting your future server system from the USB stick
				with "Try without installation" patch the file
				<code>/usr/share/ubiquity/zsys-setup</code>
				with root rights using this patch file:
				<a href="zsys-setup_22.04.patch">zsys-setup_22.04.patch</a>
			</p>
			<p>
				to apply the patch open a terminal and type
<pre>
wget https://andiweiss.github.io/projects/serverinstall/zsys-setup_22.04.patch
cd /usr/share/ubiquity
sudo patch -p1 &lt; /home/xubuntu/zsys-setup_22.04.patch
</pre>
			</p>
			<p>
				This patch modifies the zfs initialization script in a way that
				the swap partition is as large as the physical ram of the machine.
			</p>
			<p>
				After applying the patch the stick is prepared for installation.
				You can start the installation either with a new boot or without
				rebooting by using the install ubuntu icon.
			</p>
			<p>
				If you regulary work in Linux it's easy to have a look on that
				file on another linux machine. After saving the file in the
				life system shut down the life system and connect the USB stick
				to your working linux system. In a default environment your
				system will mount the stick under <code>/media/&lt;user&gt;/ventoy</code>
			</p>
			<p>
				create a directory for temporary mounting of the persitence file
<pre>
mkdir pers
</pre>
			</p>
			<p>
				Now mount the persistence file
<pre>
sudo mount /media/&lt;user&gt;/Ventoy/persistence.dat pers
</pre>
				In the case that you have chosen a different filename use that one.
			</p>
			<p>
				Now you have access to your modified zsys-setup. You find it in
				<code>pers/upper/usr/share/ubiquity/zsys-setup</code>
			</p>
		<h2 id="part_issue">Partitioning issue</h2>
			<p>
				I tried hard to get the installation doing all the things for having
				a separate data tank. But it always failed on grub installation.
				That's the reason why this howto just changes the swap partition size
				and does the additional pool handling after base installation but before
				starting the installed system the first time.
			</p>
						
	<h1 id="installation">Installation process</h1>
		<p>
			I recommend doing the installation out of the life system, not the
			<code>install xubuntu</code> possibility.
		</p>
		<p>
			Reason for that is to be able to add the data device before booting
			into the new installed system. The actual installer does not give the
			possibility to add an additional zfs pool with different features
			like mirroring or raid. The actual installer just puts everything on
			one hard disk / ssd.
		</p>
		<p>
			<em>Do the installation with only the future boot disk connected!</em>
		</p>
		<p>
			Do the installation in the regular way.
			We take care for moving the user data later.
		</p>
		<p>
			Just keep care for one Window:
		</p>
		<p>
			In the Window "Installation type" you have to chose "Advanced
			Features ...". In the now opened window select "Erase disk and use ZFS".
			Then select the device you want to boot from.
		</p>
		<p>
			Now let the installer do its work. After that you'll have an
			ubuntu system on one hard disk using ZFS wherever it is
			possible. There's only one partition with another file system.
			The EFI partition has to be formatted as VFAT.
		</p>
		<p>
			After the installation process has been finished the system asks
			for a reboot. I recommend not to do that directly but instead
			prepare the HDDs you want to use for the data pool.
		</p>
		<h2 id="additional_hdds">Initialisation of the data pool</h2>
			<p>
				After the completion of the basic installation process
				I hotplug the harddisk or harddisks for the server data pool
				while still running the life system used for installation.
				After the disks have been detected I use a commandline shell
				to create the pool and the required data sets.
			</p>
			<p>
				First step: import the already existing pools:
			</p>
<pre>
sudo zpool import -R /target rpool
sudo zpool import -R /target bpool
</pre>
			<p>
				Second step: create the data tank. For pool creation I use nearly
				the same parameters as used for rpool creation:
<pre>
sudo zpool create -f \
	-o ashift=12 \
	-o autotrim=on \
	-O compression=lz4 \
	-O acltype=posixacl \
	-O xattr=sa \
	-O relatime=on \
	-O normalization=formD \
	-O mountpoint=none \
	-O canmount=off \
	-O dnodesize=auto \
	-O sync=disabled \
	tank0 /dev/disk/by-id/&lt;your_id_here&gt;
</pre>
				Checking this against the creation of <code>rpool</code> you'll see only
				two differences:
				<ul>
					<li>the mount point is set to <code>none</code></li>
					<li>no alternative mount point is defined (no <code>-R</code> is given)</li>
				</ul>
				If the data pool shall have mirroring, raid or other fancy
				modes just hotplug the required disks and adapt the parameters to your needs.
			</p>
			<p>
				Please keep in mind:<br>
				<em>DO NOT</em> use <code>/dev/sd*</code> for the pool creation!
			</p>
			<p>
				Check the entries in <code>/dev/disk/by-id/</code> for the drives you
				want to use and use these links instead.
			</p>
			<p>
				By handing over the whole device zpool will first create a gpt partition
				table, add two partitions - one with nearly the whole content plus
				a small one containing some MB additional space. This is done to be able to
				combine drives with small differences in the amount of sectors to one pool.
				The large partition is used for zfs afterwards.
			</p>
		<h2 id="moving_userdata">Moving USERDATA</h2>
			<p>
				Third step is moving the initial user dataset. To do so we create a
				snapshot of the base installed user data set and move this to
				the new data pool.
<pre>
sudo zfs snapshot -r rpool/USERDATA@move
sudo zfs send -R rpool/USERDATA@move | sudo zfs receive tank0/USERDATA
</pre>
				Now the snapshot is available on the additional pool. This snapshot contains
				all properties of the original dataset. Parts of the properties are required
				to have the regular ubuntu zfs mechanisms working.
			</p>
			<p>
				Now the original data set and the new snapshot can be destroyed:
<pre>
sudo zfs destroy -r rpool/USERDATA
sudo zfs destroy -r tank0/USERDATA@move
</pre>
				If you only want to use the user data on the data tank you're done with the
				data movement. If you want to have server data follow the next steps and add more
				dependent on your requirements.
			</p>
		<h2 id="moving_serverdata">Moving other datasets</h2>
			<p>
				I want to have <code>/srv</code> and <code>/var/www</code>
				on my data tank.
			</p>
			<p>
				Fourth step: moving /srv on the data tank. Comparable procedure as with the
				user data, but here the installation process uses an id in the created data sets.
				So take care to replace &lt;ID&gt; by the id used in your instalation process.
				You can read it with <code>zfs list</code>
<pre>
export MyId=`zfs list | grep ROOT/ubuntu_.*/target\$ | sed -n 's|[^_]*_\(\w*\).*|\1|g;p'`
sudo zfs snapshot -r rpool/ROOT/ubuntu_${MyId}/srv@move
sudo zfs send -R rpool/ROOT/ubuntu_${MyId}/srv@move | sudo zfs receive tank0/srv
sudo zfs destroy -r rpool/ROOT/ubuntu_${MyId}/srv
sudo zfs destroy -r tank0/srv@move
</pre>
				As the mountpoint in the original dataset is inherited from the upper dataset
				we have to set the correct mountpoint here:
<pre>
sudo zfs set mountpoint=/srv tank0/srv
</pre>
			</p>
			<p>
				Fifth step: As mentioned I also want to get <code>/var/www</code> on the tank.
				There is only one difference to moving the <code>/srv</code>: I want to create
				<code>/var</code> first and then put <code>/var/www</code> on top. Just for having
				a better overview.
			</p>
			<p>
				So this is the command list for doing so:
<pre>
sudo zfs snapshot -r rpool/ROOT/ubuntu_${MyId}/var/www@move
sudo zfs create tank0/var -o canmount=off -o mountpoint=none
sudo zfs send -R rpool/ROOT/ubuntu_${MyId}/var/www@move | sudo zfs receive tank0/var/www
sudo zfs destroy -r rpool/ROOT/ubuntu_${MyId}/var/www
sudo zfs destroy -r tank0/var/www@move
sudo zfs set mountpoint=/var/www tank0/var/www
</pre>
			</p>
			<p>
				Several of the zfs properties are now set to source
				<code>received</code> instead of the usual
				<code>local</code>.
				This leads to none functional revert mechanism. If a
				revert is executed none of the datasetes having this
				source will be mounted. That's not good at all.
			</p>
			<p>
				To fix that I use a bash command line to set each
				property which has the source <code>received</code>
				on the value it has. Setting the property is done with
				<code>sudo zfs get &lt;some_parameters&gt;</code>. But
				as I don't want to do that by hand for each property
				this is the line to do it automatically in bash:
<pre>
zfs get all -r tank0 | grep received\$ | \
while read -r ds prop val drop;\
	do sudo zfs set ${prop}=${val} ${ds}; \
done
</pre>
			</p>
			<p>
				Now we have to introduce the new pool to the system as bpool and rpool are.<br/>
<pre>
sudo zpool set cachefile= tank0
sudo touch /target/etc/zfs/zfs-list.cache/tank0
sudo zfs set sync=standard tank0
</pre>
			</p>
			<p>
				Having this done tank0 is ready for usage, so now we export all pools
				in the opposite order as we imported / created them:
<pre>
sudo zpool export tank0
sudo zpool export bpool
sudo zpool export rpool
</pre>
			</p>

		<h2 id="first_boot">First boot of the fresh system</h2>
			<p>
				The system has to be rebooted. You can do that fastest with
				<code>sudo reboot</code><br/>
				You will be requested to remove the installation medium and hit enter.
			</p>
			<p>
				After doing so your system will <em>not</em> boot completly.<br/>
				Up to now it doesn't know that it has to import the new created data sets.
				But it requires these sets.
			</p>
			<p>
				I found two different behaviors:
				<ul>
					<li>You may reach the emergency state.</li>
					<li>You might get the login of your regular installation</li>
				</ul>
				In both cases the system ist not yet working.
			</p>
			<p>
				in the case that you get into the emergency mode:
			</p>
			<p>
				Hit enter again to reach a command line.<br/>
				Then type
<pre>
zpool import tank0
exit
</pre>
				Now your system boots first time into the fresh installation.
			</p>
			<p>
				In the case you get the login screen:
			</p>
			<p>
				<em>DO NOT log in.</em><br>
				Instead type &lt;ctrl&gt;-&lt;alt&gt;-&lt;F2&gt;<br>
				Now you are in a textual login. Here you can log in.
			</p>
			<p>
				Then type
<pre>
sudo zpool import tank0
sudo reboot
</pre>
				Now your system should execute the reboot and you can log in.
			</p>

	<h1 id="configuration">System configuration</h1>
		<p>
			The initial system installation is done now. There will be updates
			available - these can either be installed first or last. The result
			will be the same.
		</p>

		<h2>ssh server</h2>
			<p>
				Now the system has a state where I want to get remote access.
				Therefore my next step is the installation of the ssh server.
<pre>
sudo apt install openssh-server
</pre>
			</p>
			<p>
				In the case you want to replace an old server it may make sense
				to overtake the ssh keys from the old machine.<br>
				To do so copy the files <code>/etc/ssh/*key*</code> from the
				old machine to the new one.
			</p>

		<h2 id="small_issues">Small configuration issues</h2>
			<p>
				One thing I really like is the possibility to use pageup /
				pagedown for searching the history.<br/>
				This can be enabled in <code>/etc/inputrc</code><br>
				Just search for the lines and remove the hash.
			</p>
			<p>
				As last little config issue I disable the bluetooth service.<br>
				It usually crashes during boot and I don't have a bluetooth
				device connected to the server.
<pre>
sudo systemctl disable bluetooth
</pre>
			</p>

		<h2 id="supend_hibernate_config">Configuration of suspend and hibernate</h2>
			<p>
				Now the basics are done. We have a system booting from ZFS and
				the desktop is clean.
			</p>
			<p>
				Next step to do is the configuration of suspend to RAM and
				suspend to disk.
			</p>
			<p>
				Ubuntu 22.04 has all required things installed to do suspend.<br>
				To test it use <code>sudo systemctl suspend</code> and the system
				should got in suspend to ram.
			</p>
			<p>
				To get hibernate working we have to modify the kernel command line
				and modify the initramfs.
			</p>
			<ul>
				<li>get the uuid of your swap partition<br>
				easiest way: check directory <code>/dev/disk/by-uuid</code><br>
				check for the link pointing to the second partition of your boot device</li>
				<li>edit the file <code>/etc/default/grub</code><br>
				add <code>resume=UUID=&lt;your_uuid&gt;</code><br>
				to the tag <code>GRUB_CMDLINE_LINUX_DEFAULT</code></li>
				<li>edit or create the file <code>/etc/initramfs-tools/conf.d/resume</code><br>
				add the line <code>RESUME=UUID=&lt;your_uuid&gt;</code></li>
				<li>execute <code>sudo update-grub</code></li>
				<li>execute <code>sudo update-initramfs -u -k all</code></li>
			</ul>
			<p>
				Now you can check hibernation.<br>
				execute <code>sudo systemctl hibernate</code><br>
				and your system should switch off.
			</p>
			<p>
				During next boot the kernel should report that it's recovering from your
				swap partition.<br>
				The screen should be locked and after logging in
				the systen should have the same state as before hibernation.
			</p>
			<p>
				To have this as a sensefull feature you have to setup your BIOS
				in the way that after power loss the computer starts up.<br>
				In most cases the default for this setting is "keep it off".
			</p>

		<h2 id="wol_config">Configuration of Wake On Lan</h2>
			<p>
				As already explained in the prerequesites the Atheros AR8171 in
				principle supports Wake On Lan, but the related part of the driver
				has been taken out of the kernel.<br>
				This driver is build as module. Its name is <code>alx</code>.
			</p>
			<p>
				First thing to do is to install ethtool, git and dkms:
<pre>
sudo apt install ethtool git dkms
</pre>
			</p>
			<p>
				Now we need to know which interface we want to configure:
<pre>
ip link
</pre>
				We identify the interface, in my case this is <code>enp2s0</code>
			</p>
			<p>
				Next step is to check the supported wake on lan modes and the
				actually configured ones:
<pre>
sudo ethtool enp2s0 | grep Wake-on
</pre>
				On my server hardware this report is empty.<br>
				The original kernel driver doesn't support Wake On Lan.
			</p>
			<p>
				This is sad, because this means: No Wake On Lan supported ...
			</p>
			<p>
				<em>But:</em> one can still use an old version of a driver to analyse
				the code and take the parts required for wol and patch them into the
				current driver.<br>
				I did some analysis on the kernel driver source and found out that
				there is no big change from kernel 5.15 to kernel 6.03. So I created
				a dkms package to compile an alx driver with wol feature on each new
				installed kernel.
			</p>
			<p>
				This dkms package can be found here:<br>
				<a href="https://github.com/AndiWeiss/alx-wol">dkms package for alx with wake on lan</a><br>
				Best way to use it is to clone it and rund the installation script:
<pre>
git clone https://github.com/AndiWeiss/alx-wol.git
cd alx-wol
sudo ./install_alx-wol.sh
</pre>
			</p>
			<p>
				This script does the complete installation of the dkms and also
				replaces the original module of the running system by the one
				supporting wol. To check that use the command
<pre>
sudo ethtool enp2s0 | grep Wake-on
</pre>
				again.
			</p>
			<p>
				The new result is:
<pre>
Supports Wake-on: pg
Wake-on: pg
</pre>
			</p>
			<p>
				So now Wake On Lan is activated on
				<ul>
					<li>phy activity</li>
					<li>MagicPacket</li>
				</ul>
				I don't like phy activity waking up my system, so I have to
				use ethtool to change that behavior. Changing it in the
				commandline will be lost after reboot, therefore I add a task
				in systemd.
			</p>
			<p>
				To create that service create the file
				<code>/etc/systemd/system/wol.service</code>
				with root rights. This file has to contain:
<pre>
[Unit]
Description=Configure Wake-up on LAN

[Service]
Type=oneshot
ExecStart=/sbin/ethtool -s enp2s0 wol g

[Install]
WantedBy=basic.target
</pre>
				Then enable the service with
<pre>
sudo systemctl enable wol.service
</pre>
			</p>
			<p>
				After a reboot ethtool reports
<pre>
Supports Wake-on: pg
Wake-on: g
</pre>
				<em>This</em> is what we want to get.
			</p>
			<p>
				Testing the feature can be done with another linux system.
				I recommend to use wakeonlan which has to be installed with
<pre>
sudo apt install wakeonlan
</pre>
				on another linux machine in the same network.
			</p>
			<p>
				The previous used command <code>ip link</code> does not only
				print the interface name. It also prints the mac address. The
				mac address is the part behind <code>link/ether</code>,
				containing six hexadecimal values with 2 digits each, the values
				separated by colons.
			</p>
			<p>
				To try if wake on lan is functional put your server to sleep
				e.g. with <code>sudo systemctl suspend</code>.<br>
				Then, on the other machine, use
<pre>
wakeonlan 11:22:33:44:55:66
</pre>
				to wake it up again.
				Take care to use the mac address of your server.<br>
				Another thing you need to know is that the magic packet used
				for waking up a system is not routed over wifi, so you have
				to use a computer connected to the lan.
			</p>

		<h2 id="share_export">exporting nfs shares</h2>
			<p>
				Now we have a system providing the basic system features.
				Now the server featurs have to be activated.
				I prefer haveing nfs shares, so I don't take cifs shared into
				account here.
			</p>
			<p>
				First thing to prepare nfs shares is the installtion of the
				nfs server:
<pre>
sudo apt install nfs-kernel-server
</pre>
			</p>
			<p>
				I decided to use the zfs mechanisms for exporting nfs shares.
				Therefore exporting the filesystems is done with zfs properties,
				not in <code>/etc/exports</code>.
			</p>
			<p>
				I'm going to show the export with my folder containing the mp3
				collection. This is located in <code>/srv/music</code>
			</p>
			<p>
				First I have to create that dataset:
<pre>
sudo zfs create tank0/srv/music
</pre>
				By creating this dataset there's automatically a directory
				<code>/srv/music</code>
				available. Take care to set the access (user, group, read,
				write, execute, ...) according your needs.
			</p>
			<p>
				Now we simply tell zfs to export this filesystem:
<pre>
sudo zfs set sharenfs=on tank0/srv/music
</pre>
				After doing so this share can be mounted on another linux
				machine:
<pre>
sudo mount server:/srv/music music
</pre>
			</p>

	<h1 id="shutdown">Automatic shutdown</h1>
		<p>
			Now the basic system is complete.
			User can be added and their home directory will be created
			on the tank device. Suspend to ram and disk is working.
			Wake on LAN is also full functional.
			So the next thing is to shutdown the system automatically.
		</p>
		<p>
			Actually I see two things which have to keep the system alive:
			<ul>
				<li>any nfs share in use</li>
				<li>a user logged in via ssh or in the terminal</li>
			</ul>
		</p>
		<p>
			But before shutting down the system it shall be checked if there are
			updates. If yes these updates shall be installed. And if - after
			this update - the system requires a reboot this shall be done, too.
		</p>

		<h2 id="nfs_detection">Detection of nfs mounts</h2>
			<p>
				To detect how many nfs mounts are actually in use I use the
				program netstat. This is included in the ubuntu package
				net-tools, which is not installed as default.<br>
				So we have to install it:
<pre>
sudo apt install net-tools
</pre>
			</p>
			<p>
				To get a list of active nfs connections I use
<pre>
netstat -n --inet
</pre>
				I use the <code>-n</code> to get numeric output. Otherwise there has
				to be a functional nameserver in your network. Additionally we need
				the ip address of the client lateron.
			</p>
			<p>
				This command does not only print the nfs connections, it prints all
				active tcp and udp connections. So we have to filer for nfs.
			</p>
			<p>
				Taking a look into <code>/etc/services</code> we find the ports used for
				nfs. This is 2049. With this I use
<pre>
netstat -n --inet | grep ':2049\W'
</pre>
				to get only the lines with active nfs connections.
			</p>
			<p>
				During my tries I found an issue for generic usage:<br/>
				If the system using the share doesn't unmount the share it can happen that
				netstat detects the link active for a long time. So we also have to check if
				the system with the actual link is still alive.
			</p>
			<p>
				For the correct handling of this we need a script.
				Here I don't quote the script, you'll find it where the cron job is explained.
				I'll just explain how that works.
			</p>
			<p>
				As long as the system is alive we expect that it can be ping'ed.
				This depends if there is a firewall installed which blocks the ping,
				on my system this is not the case. So I can use ping.
			</p>
			<p>
				To be able to ping the system we need its ip address.
				To get this we use the output of the found line of netstat and kill everything but the ip address:
<pre>
netstat -n --inet | grep ':2049\W' | sed -n 's|.*:2049\W*\([^:]*\):.*|\1|g;p'
</pre>
			</p>

		<h2 id="user_detection">Detection of active users</h2>
			<p>
				Getting the information if users are logged in is easy:
<pre>
users
</pre>
				prints all users who are logged in.
			</p>

		<h2 id="updates">Checking for updates</h2>
			<p>
				Easiest thing is not checking for updates but just doing the updates.
				To do so we tell the system to
				<ul>
					<li>get the new package list</li>
					<li>install the security updates</li>
					<li>install the updates which don't require interaction</li>
					<li>do an autoclean</li>
				</ul>
			</p>

		<h2 id="reboot">Reboot if required</h2>
			<p>
				The update process creates a file <code>/var/run/reboot-required</code>
				if a reboot is required.<br>
				If this file is detected the server will - instead of
				go to suspend - do a reboot.
			</p>

		<h2 id="maintenance">Wake up for maintenance</h2>
			<p>
				In my case the server is often not used for a long time.
				To avoid having a too long tim without updates the system
				shall wake up at least once a week and do maintenance.
			</p>
			<p>
				To get that there is a tool called <code>rtcwakeup</code> which
				uses the rtc alarm to start the system at a defined time.
				My decission is to start the system at the same day in one week
				at 4 o'clock in the morning.
			</p>
			<p>
				This is done in the script before shutting down the system
				with the command
<pre>
rtcwake -m no -t `date --date='7 days 04:00' +%s`
</pre>
				This sets the alarm in the rtc which then will start up the system.
			</p>

	<h2 id="all_together">Putting it together</h3>
		<p>
			Now we know how to get the information if we have to shut down.
			These things have to be used for putting it all together.
			We need to know if a user is actually logged in and if a share is
			actually in use.<br/>
			If this is not the case for a certain time the system shall shut down.
		</p>
		<p>
			As this is time based we create a cron job for that.
			In the cron job, once per minute, we collect the information.
			Then we store the information when we first have no keep alive
			status anymore. If this doesn't change for the defined time we
			do a <code>s2both</code> and the system gets suspended.
		</p>
		<p>
			I provide a script for the automatic shutdown:<br/>
			<a href="autosuspend.sh">autosuspend.sh</a><br/>
			Save this script with root rights as <code>/bin/autosuspend.sh</code>
			and don't forget to <code>sudo chmod +x /bin/autosuspend.sh</code>
			to make it executable.
<pre>
wget https://AndiWeiss.github.io/projects/serverinstall/autosuspend.sh
chmod a+x autosuspend.sh
sudo chown root:root autosuspend.sh
sudo mv autosuspend.sh /bin
</pre>
			After that we create a cronjob calling this script every minute.
			Therefore open your <code>/etc/crontab</code> with root rights
			and add the line
<pre>
* * * * * root /bin/autosuspend.sh
</pre>
			With this the script is executed each minute and then checks
			if the system shall stay active or not.
		</p>
		<p>
			To check the autosuspend activities a log file
			<code>/var/log/autosuspend.log</code> is created.
		</p>

	<h1 id="crash_recovery">Crash recovery</h1>
		<p>
			The best server is senseless if there is no possibility to do crash recovery.
		</p>
		<p>
			I see the following things which have to be easy and fast recoverable:
			<ul>
				<li>crash of the boot system<br>
				The boot system is without raid or comparable things. Therefore
				the likelyhood for a data loss is higher than on the data tank.</li>
				<li>crash of one tank0 disc<br>
				I personally use a zfs raidz for my tank0. So as long as only one
				drive fails all the data is still available. But I need to know in
				advance what to do in the case that a drive crashes.</li>
			</ul>
		</p>

	<h2 id="crashed_boot_system">Recovery of a crashed boot system</h1>
		<p>
			I decided to define the way of setting up a fresh system in the case that
			the boot system crashes. I completely separated the server data and user
			data from the boot system so I only need to be sure that there is an easy and
			fast possibility to get a clean system working with the still existing tank0.
		</p>
		<p>
			This method is rather simple:
			<ul>
				<li>disconnect all tank0 drives from the server</li>
				<li>do a plain zfs installation on the new boot device<br>
				For this just follow the description above</li>
				<li>Instead of creating a new data pool only destroy the parts
				on the fresh installation</li>
				<li>follow the 'first boot' instructions</li>
				<li>now install the backup of your configuration<br>
				I hope you have that backup :-)</li>
			</ul>
		</p>

	<h2 id="crashed_tank_hdd">Recovery of a crashed tank0 hard disc</h1>
		<p>up to now not collected. Follow the standard zfs recovery documentation.</p>
</body>
</html>

<!-- wget https://AndiWeiss.github.io/projects/serverinstall/install_base.sh -->
<!-- wget https://AndiWeiss.github.io/projects/serverinstall/install_final.sh -->
